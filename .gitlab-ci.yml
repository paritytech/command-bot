variables:
  REPO: /repo
  IMAGE_REPOSITORY: docker.io/paritytech/try-runtime-bot
  CI_IMAGE: ${IMAGE_REPOSITORY}:ci
  KUBE_NAMESPACE: try-runtime
  DEPLOYMENT_TAG: ${CI_ENVIRONMENT_NAME}${CI_COMMIT_TAG}
  DEPLOYMENT_CONTAINER: try-runtime
  DATA_PATH: /data
  CLEAR_DB_ON_START: "false"
  CLEAR_REPOSITORIES_ON_START: "false"
  CLEAR_TMPDIR_ON_START: "true"
  PING_PORT: 3001

default:
  image: $CI_IMAGE
  tags:
    -  kubernetes-parity-build

.on-pr-and-master: &on-pr-and-master
  rules:
    - if: $BUILD
      when: never
    - if: $CI_COMMIT_REF_NAME == "master"
    - if: $CI_COMMIT_REF_NAME =~ /^[0-9]+$/                         # PRs

.on-tags: &on-tags
  rules:
    - if: $CI_COMMIT_REF_NAME =~ /^v-[0-9]+\.[0-9]+.*$/             # i.e. v-1.0, v-2.1rc1

.on-master: &on-master
  rules:
    - if: $BUILD
      when: never
    - if: $CI_COMMIT_REF_NAME == "master"

stages:
  - install_deps
  - lint
  - after_lint
  - containerize
  - deploy

install-deps:
  <<: *on-pr-and-master
  stage: install_deps
  script:
    - yarn install --frozen-lockfile
  artifacts:
    name:       "${CI_JOB_ID}_${CI_COMMIT_REF_NAME}"
    when:       on_success
    expire_in:  1 hour
    paths:
      - ./node_modules

lint:
  <<: *on-pr-and-master
  stage: lint
  script:
    - yarn lint

pre_commit:
  <<: *on-pr-and-master
  stage: lint
  script:
    - pre-commit run --color=always --all

build_bot:
  <<: *on-pr-and-master
  stage: after_lint
  script:
    - yarn build

#### App deployment

.kubernetes: &kubernetes
  image: paritytech/kubetools:helm3

.build-app-image: &build-app-image
  image: quay.io/buildah/stable
  script:
    - echo "$Docker_Hub_Pass_Parity" |
      buildah login --username "$Docker_Hub_User_Parity" --password-stdin docker.io
    - ./scripts/generateDockerfile app |
      buildah bud
      --format=docker
      --build-arg "REPO=$REPO"
      -v "$PWD:$REPO"
      --tag "$IMAGE_REPOSITORY:$DEPLOYMENT_TAG"
      -
    - buildah push --format=v2s2 "$IMAGE_REPOSITORY:$DEPLOYMENT_TAG"
  after_script:
    - buildah logout --all

.deploy-k8s: &deploy-k8s
  <<: *kubernetes
  interruptible: true
  script:
    # Change the app version during build so that Kubernetes is able to update
    # the deployment even if the images' tags did not change
    # $CI_PIPELINE_ID is guaranteed to be unique
    - export KUBERNETES_VERSION_TAG="$CI_PIPELINE_ID"
    # Those values are embedded in the annotations and that's how the change is
    # noticed
    - |-
      sed -i "s/version:.*/version: $KUBERNETES_VERSION_TAG/" helm/Chart.yaml
    - |-
      sed -i "s/appVersion:.*/appVersion: $KUBERNETES_VERSION_TAG/" helm/Chart.yaml
    - helm upgrade --install "$KUBE_NAMESPACE" ./helm
      --values helm/values.yaml
      --namespace "$KUBE_NAMESPACE"
      --set image.repository="$IMAGE_REPOSITORY"
      --set image.tag="$DEPLOYMENT_TAG"
      --set env.APP_ID="$APP_ID"
      --set env.CLIENT_ID="$CLIENT_ID"
      --set env.CLIENT_SECRET="$CLIENT_SECRET"
      --set env.WEBHOOK_SECRET="$WEBHOOK_SECRET"
      --set env.PRIVATE_KEY_BASE64="$PRIVATE_KEY_BASE64"
      --set env.ALLOWED_ORGANIZATIONS="$ALLOWED_ORGANIZATIONS"
      --set env.DATA_PATH="$DATA_PATH"
      --set persistence.mountPath="$DATA_PATH"
      --set env.IS_DEPLOYMENT="true"
      --set env.DEPLOYMENT_ENVIRONMENT="$CI_ENVIRONMENT_NAME"
      --set env.DEPLOYMENT_CONTAINER="$DEPLOYMENT_CONTAINER"
      --set env.CLEAR_DB_ON_START="$CLEAR_DB_ON_START"
      --set env.CLEAR_REPOSITORIES_ON_START="$CLEAR_REPOSITORIES_ON_START"
      --set env.TMPDIR="$DATA_PATH/tmp"
      --set env.CLEAR_TMPDIR_ON_START="$CLEAR_TMPDIR_ON_START"
      --set env.PING_PORT="$PING_PORT"
      --set env.MATRIX_HOMESERVER="$MATRIX_HOMESERVER"
      --set env.MATRIX_ACCESS_TOKEN="$MATRIX_ACCESS_TOKEN"
      --set env.MASTER_TOKEN="$MASTER_TOKEN"
      --set env.POLKADOT_WEBSOCKET_ADDRESS="$POLKADOT_WEBSOCKET_ADDRESS"
      --set env.KUSAMA_WEBSOCKET_ADDRESS="$KUSAMA_WEBSOCKET_ADDRESS"
      --set env.WESTEND_WEBSOCKET_ADDRESS="$WESTEND_WEBSOCKET_ADDRESS"
      --set env.LOG_FORMAT=json

.uninstall-deployment: &uninstall-deployment
  <<: *kubernetes
  stage: deploy
  interruptible: true
  script:
    - helm uninstall "$KUBE_NAMESPACE" --namespace "$KUBE_NAMESPACE"

#### > Production deployment

#### >> Manual

.production-env: &production-env
  environment:
    name: parity-chains

build-production-image-manual:
  <<: *build-app-image
  <<: *production-env
  stage: containerize
  rules:
    - if: "$BUILD == 'production'"

deploy-production-manual:
  <<: *deploy-k8s
  <<: *production-env
  stage: deploy
  rules:
    - if: '$BUILD == "production" || $DEPLOY == "production"'

uninstall-production:
  <<: *uninstall-deployment
  <<: *production-env
  rules:
    - if: "$UNINSTALL == 'production'"

#### >> Automatic

.tagged-production-build: &tagged-production-build
  <<: *production-env
  <<: *on-tags

build-production-image:
  <<: *build-app-image
  <<: *tagged-production-build
  stage: containerize

deploy-production:
  <<: *deploy-k8s
  <<: *tagged-production-build
  stage: deploy

#### CI images

.build-ci-image: &build-ci-image
  image: quay.io/buildah/stable
  script:
    - echo "$Docker_Hub_Pass_Parity" |
      buildah login --username "$Docker_Hub_User_Parity" --password-stdin docker.io
    - ./scripts/generateDockerfile ci |
      buildah bud
      --format=docker
      --tag "$CI_IMAGE" -
    - buildah push --format=v2s2 "$CI_IMAGE"
  after_script:
    - buildah logout --all

build-ci-image-manual:
  <<: *build-ci-image
  stage: containerize
  rules:
    - if: "$BUILD == 'ci'"

build-ci-image:
  <<: *on-master
  <<: *build-ci-image
  stage: containerize
  allow_failure: true
